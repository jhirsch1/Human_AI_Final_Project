{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "s3RVSe0IFrG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "from typing import Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_bIRUqpElMsy"
      },
      "outputs": [],
      "source": [
        "# helpful enums\n",
        "\n",
        "# object types - bottle, can, or glass\n",
        "class ObjectType(Enum):\n",
        "    BOTTLE = 1\n",
        "    CAN = 2\n",
        "    GLASS = 3\n",
        "\n",
        "# outcomes\n",
        "class OutcomeType(Enum):\n",
        "    SP_SUCCESS = 0 # robot succeeds\n",
        "    SP_FAIL = 1 # robot fails\n",
        "    IT = 2 # human intervenes\n",
        "\n",
        "TRUST_LEVELS = [1,2,3,4,5,6,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Nc_rrkUolvOp"
      },
      "outputs": [],
      "source": [
        "# reward table\n",
        "\n",
        "REWARD_TABLE: Dict[Tuple[ObjectType, OutcomeType], float] = {\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_SUCCESS): 1.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_FAIL):    0.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.IT):         0.0,\n",
        "\n",
        "    (ObjectType.CAN, OutcomeType.SP_SUCCESS):    2.0,\n",
        "    (ObjectType.CAN, OutcomeType.SP_FAIL):      -4.0,\n",
        "    (ObjectType.CAN, OutcomeType.IT):            0.0,\n",
        "\n",
        "    (ObjectType.GLASS, OutcomeType.SP_SUCCESS):  3.0,\n",
        "    (ObjectType.GLASS, OutcomeType.SP_FAIL):    -9.0,\n",
        "    (ObjectType.GLASS, OutcomeType.IT):          0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6gQt-AkTl4O8"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class HumanDecisionModel:\n",
        "\n",
        "  # maps object type to discounted reward --> must learn this from data\n",
        "  gamma: Dict[ObjectType, float]\n",
        "  # maps object type to 'risk penalty' --> must learn this from data\n",
        "  eta: Dict[ObjectType, float]\n",
        "\n",
        "  # does the human think the robot will succeed?\n",
        "  # bj = S(gamma*theta + eta)\n",
        "  def success_belief(self, theta, obj):\n",
        "\n",
        "    g = self.gamma[obj]\n",
        "    n = self.eta[obj]\n",
        "\n",
        "    return sigmoid(g * theta + n)\n",
        "\n",
        "  # what is the probability the human will stay put / not intervene?\n",
        "  # Pt = S(bj*rj + (1 - bj)*rj)\n",
        "  def prob_stay_put(self, theta, obj):\n",
        "\n",
        "    b_tj = self.success_belief(theta, obj)\n",
        "    rS = REWARD_TABLE[(obj, OutcomeType.SP_SUCCESS)]\n",
        "    rF = REWARD_TABLE[(obj, OutcomeType.SP_FAIL)]\n",
        "\n",
        "    return sigmoid(b_tj * rS + (1 - b_tj) * rF)\n",
        "\n",
        "  # calculate human action\n",
        "  # a ~ B(Pt)\n",
        "  def sample_human_action(self, theta, obj):\n",
        "\n",
        "    p_stay = self.prob_stay_put(theta, obj)\n",
        "    if np.random.rand() < p_stay:\n",
        "      return OutcomeType.SP_SUCCESS\n",
        "    return OutcomeType.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "JtvSZsE20NLq"
      },
      "outputs": [],
      "source": [
        "human_model = HumanDecisionModel()\n",
        "\n",
        "# bayesion inference for the probability of each observation\n",
        "def belief(state_theta, action_obj, observation):\n",
        "\n",
        "    p_stay = human_model.prob_stay_put(state_theta, action_obj)\n",
        "\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        return p_stay\n",
        "    else:  # observation == OutcomeType.IT (OutcomeType.Fail isn't possible yet)\n",
        "        return 1 - p_stay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "16qr31Uu4U4S"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def belief_dist(theta_t, observation, alpha=1, beta=.2, sigma=1):\n",
        "    \"\"\"\n",
        "    P(theta_{t+1} | theta_t, a_t, o_t)\n",
        "\n",
        "    theta ~ N(u, sigma)\n",
        "    where u changes as shown below:\n",
        "    - if robot succeeds:      u + alpha\n",
        "    - if human intervenes:    u - beta\n",
        "    \"\"\"\n",
        "\n",
        "    # if the robot succeeds\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        mean = theta_t + alpha  # trust increases\n",
        "    else:  # otherwise, trust decreases slightly (by beta)\n",
        "        mean = theta_t - beta\n",
        "\n",
        "    # calculate probabilities for all trust levels\n",
        "    # ex. if the robot succeeds, higher trust level becomes more probably\n",
        "    probs = {}\n",
        "    for theta in TRUST_LEVELS:\n",
        "        lower = theta - 0.5\n",
        "        upper = theta + 0.5\n",
        "        probs[theta] = norm.cdf(upper, loc=mean, scale=sigma) - norm.cdf(lower, loc=mean, scale=sigma)\n",
        "\n",
        "    total = sum(probs.values())\n",
        "    probs = {theta: p / total for theta, p in probs.items()}\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "u0vCxyptn1vQ"
      },
      "outputs": [],
      "source": [
        "def belief_update(belief, action, observation, human_model):\n",
        "    \"\"\"\n",
        "    q(theta) = sum_over_all_t(b(theta)p(next_theta | theta, a_r, a_h)) \n",
        "\n",
        "    -- typical belief update equation for POMDPs\n",
        "    \"\"\"\n",
        "    predicted = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        predicted[theta_next] = 0.0\n",
        "        for theta_curr in TRUST_LEVELS:\n",
        "            prior = belief[theta_curr]\n",
        "            trans_prob = belief_dist(theta_curr, observation)[theta_next]\n",
        "            predicted[theta_next] += prior * trans_prob\n",
        "\n",
        "    new_belief = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        if observation == OutcomeType.SP_SUCCESS:\n",
        "            obs_prob = human_model.prob_stay_put(theta_next, action)\n",
        "        else:\n",
        "            obs_prob = 1 - human_model.prob_stay_put(theta_next, action)\n",
        "\n",
        "        new_belief[theta_next] = predicted[theta_next] * obs_prob\n",
        "\n",
        "    total = sum(new_belief.values())\n",
        "    new_belief = {theta: p / total for theta, p in new_belief.items()}\n",
        "\n",
        "    return new_belief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QwNPIreDEmY1"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "# ChatGPT helped generate an exhaustive tree search in place of using SARSOP (which was incredibly difficult to download/run)\n",
        "# The important distinction here is that, since our POMDP environment is so small, we are able to explore the entire state space\n",
        "# Our results converge to a completely optimal policy (versus SARSOP uses approximation)\n",
        "# This would NOT scale to a larger environment\n",
        "\n",
        "def simple_trust_aware_policy(belief, human_model, available_actions):\n",
        "    \"\"\"\n",
        "    Full-horizon trust-aware planning with memoization.\n",
        "    Explores all trajectories until remaining_objects is empty.\n",
        "    Returns (best_action, best_total_expected_reward).\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert initial belief to tuple (because dicts aren't hashable)\n",
        "    init_belief_tuple = tuple(belief[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    # Convert available_actions to a tuple of integers (ObjectType values)\n",
        "    init_actions_tuple = tuple(sorted([obj.value for obj in available_actions]))\n",
        "\n",
        "    # ---- 1. One-step expected reward (unchanged) ----\n",
        "    def immediate_expected_reward(belief_state, action_obj):\n",
        "        expected = 0.0\n",
        "        for theta in TRUST_LEVELS:\n",
        "            p_theta = belief_state[theta]\n",
        "            p_stay = human_model.prob_stay_put(theta, action_obj)\n",
        "\n",
        "            reward_if_stay = REWARD_TABLE[(action_obj, OutcomeType.SP_SUCCESS)]\n",
        "            reward_if_intervene = REWARD_TABLE[(action_obj, OutcomeType.IT)]\n",
        "\n",
        "            expected += p_theta * (\n",
        "                p_stay * reward_if_stay +\n",
        "                (1 - p_stay) * reward_if_intervene\n",
        "            )\n",
        "        return expected\n",
        "\n",
        "    # ---- 2. Cached wrapper for belief_update ----\n",
        "    @lru_cache(maxsize=None)\n",
        "    def cached_belief_update(belief_tuple, action_value, obs_value):\n",
        "        belief_dict = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "        action_obj = ObjectType(action_value)\n",
        "        obs = OutcomeType(obs_value)\n",
        "        new_b = belief_update(belief_dict, action_obj, obs, human_model)\n",
        "        return tuple(new_b[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    # ---- 3. Main recursive value function, fully memoized ----\n",
        "    @lru_cache(maxsize=None)\n",
        "    def expected_return(belief_tuple, actions_tuple):\n",
        "        \"\"\"\n",
        "        belief_tuple: tuple of 7 floats (belief over trust levels)\n",
        "        actions_tuple: sorted tuple of remaining object values (ints)\n",
        "        \"\"\"\n",
        "\n",
        "        # Base case: no objects left\n",
        "        if not actions_tuple:\n",
        "            return (None, 0.0)\n",
        "\n",
        "        # Convert tuple â†’ usable dict\n",
        "        belief_state = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "\n",
        "        best_act = None\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        # Try each remaining action\n",
        "        for idx, action_value in enumerate(actions_tuple):\n",
        "            action_obj = ObjectType(action_value)\n",
        "\n",
        "            # 1-step reward\n",
        "            immediate = immediate_expected_reward(belief_state, action_obj)\n",
        "\n",
        "            # Expected future reward\n",
        "            future = 0.0\n",
        "            for obs in (OutcomeType.SP_SUCCESS, OutcomeType.IT):\n",
        "                obs_value = obs.value\n",
        "\n",
        "                # Compute P(obs | belief, action)\n",
        "                if obs == OutcomeType.SP_SUCCESS:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] * human_model.prob_stay_put(t, action_obj)\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                else:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] * (1 - human_model.prob_stay_put(t, action_obj))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "\n",
        "                if p_obs == 0:\n",
        "                    continue\n",
        "\n",
        "                # Updated belief (cached!)\n",
        "                next_b_tuple = cached_belief_update(belief_tuple, action_value, obs_value)\n",
        "\n",
        "                # Remove this action from remaining list\n",
        "                next_actions = list(actions_tuple)\n",
        "                next_actions.pop(idx)\n",
        "                next_actions_tuple = tuple(sorted(next_actions))\n",
        "\n",
        "                # Recursive call (cached!)\n",
        "                _, v_next = expected_return(next_b_tuple, next_actions_tuple)\n",
        "\n",
        "                future += p_obs * v_next\n",
        "\n",
        "            total = immediate + future\n",
        "\n",
        "            if total > best_val:\n",
        "                best_val = total\n",
        "                best_act = action_obj\n",
        "\n",
        "        return (best_act, best_val)\n",
        "\n",
        "    # ---- Call planner ----\n",
        "    return expected_return(init_belief_tuple, init_actions_tuple)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Zv0TtyvUEtaA"
      },
      "outputs": [],
      "source": [
        "def simulate_task(human_model, initial_belief, initial_theta):\n",
        "    \"\"\"\n",
        "    simulating full table-clearing task\n",
        "    \"\"\"\n",
        "    \n",
        "    belief = initial_belief.copy()\n",
        "    total_reward = 0.0\n",
        "    history = []\n",
        "\n",
        "    true_theta = initial_theta\n",
        "\n",
        "    remaining_objects = [\n",
        "        ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "        ObjectType.CAN, ObjectType.GLASS\n",
        "    ]\n",
        "\n",
        "    while remaining_objects:\n",
        "        \n",
        "        # robot action\n",
        "        action, _ = simple_trust_aware_policy(belief, human_model, remaining_objects)\n",
        "\n",
        "        # observes whether human intervenes\n",
        "        observation = human_model.sample_human_action(true_theta, action)\n",
        "\n",
        "        # get reward\n",
        "        reward = REWARD_TABLE[(action, observation)]\n",
        "        total_reward += reward\n",
        "\n",
        "        # trust dynamics\n",
        "        trans_probs = belief_dist(true_theta, observation)\n",
        "        true_theta = np.random.choice(\n",
        "            TRUST_LEVELS,\n",
        "            p=[trans_probs[theta] for theta in TRUST_LEVELS]\n",
        "        )\n",
        "\n",
        "        # remove action availability\n",
        "        remaining_objects.remove(action)\n",
        "\n",
        "        history.append(action.name)\n",
        "\n",
        "    return total_reward, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG4Sd-OXG4nO",
        "outputId": "34ed5d50-3382-4e4e-ab90-857b27307b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Reward: 8.0\n",
            "Action Sequence: ['BOTTLE', 'BOTTLE', 'BOTTLE', 'CAN', 'GLASS']\n"
          ]
        }
      ],
      "source": [
        "# FULL RUN\n",
        "\n",
        "human_model = HumanDecisionModel()\n",
        "\n",
        "human_model.gamma = {\n",
        "    ObjectType.BOTTLE: 1,   \n",
        "    ObjectType.CAN:    1.2,  \n",
        "    ObjectType.GLASS:  1.5,  \n",
        "}\n",
        "\n",
        "human_model.eta = {\n",
        "    ObjectType.BOTTLE: 0,  \n",
        "    ObjectType.CAN:   -2.5, \n",
        "    ObjectType.GLASS: -5,  \n",
        "}\n",
        "\n",
        "initial_belief =   {\n",
        "    1: 0.0625,\n",
        "    2: 0.125,\n",
        "    3: 0.1825,\n",
        "    4: 0.375,\n",
        "    5: 0.1825,\n",
        "    6: 0.125,\n",
        "    7: 0.0625\n",
        "}\n",
        "\n",
        "total_reward, history = simulate_task(\n",
        "            human_model,\n",
        "            initial_belief,\n",
        "            4\n",
        ")\n",
        "\n",
        "print(\"Total Reward: \" + str(total_reward))\n",
        "print(\"Action Sequence: \" + str(history))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "csci1470 (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
