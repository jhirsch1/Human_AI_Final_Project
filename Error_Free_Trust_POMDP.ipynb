{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3RVSe0IFrG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import Enum, auto\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bIRUqpElMsy"
      },
      "outputs": [],
      "source": [
        "# helpful enums\n",
        "\n",
        "# object types - bottle, can, or glass\n",
        "class ObjectType(Enum):\n",
        "    BOTTLE = 1\n",
        "    CAN = 2\n",
        "    GLASS = 3\n",
        "\n",
        "# outcomes\n",
        "class OutcomeType(Enum):\n",
        "    SP_SUCCESS = 0 # robot succeeds\n",
        "    SP_FAIL = 1 # robot fails\n",
        "    IT = 2 # human intervenes\n",
        "\n",
        "TRUST_LEVELS = [1,2,3,4,5,6,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc_rrkUolvOp"
      },
      "outputs": [],
      "source": [
        "# reward table\n",
        "\n",
        "REWARD_TABLE: Dict[Tuple[ObjectType, OutcomeType], float] = {\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_SUCCESS): 1.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_FAIL):    0.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.IT):         0.0,\n",
        "\n",
        "    (ObjectType.CAN, OutcomeType.SP_SUCCESS):    2.0,\n",
        "    (ObjectType.CAN, OutcomeType.SP_FAIL):      -4.0,\n",
        "    (ObjectType.CAN, OutcomeType.IT):            0.0,\n",
        "\n",
        "    (ObjectType.GLASS, OutcomeType.SP_SUCCESS):  3.0,\n",
        "    (ObjectType.GLASS, OutcomeType.SP_FAIL):    -9.0,\n",
        "    (ObjectType.GLASS, OutcomeType.IT):          0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gQt-AkTl4O8"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class HumanDecisionModel:\n",
        "\n",
        "  # maps object type to discounted reward --> must learn this from data\n",
        "  gamma: Dict[ObjectType, float]\n",
        "  # maps object type to 'risk penalty' --> must learn this from data\n",
        "  eta: Dict[ObjectType, float]\n",
        "\n",
        "  # does the human think the robot will succeed?\n",
        "  def success_belief(self, theta, obj):\n",
        "\n",
        "    g = self.gamma[obj]\n",
        "    n = self.eta[obj]\n",
        "\n",
        "    return sigmoid(g * theta + n)\n",
        "\n",
        "  # what is the probability the human will stay put / not intervene?\n",
        "  def prob_stay_put(self, theta, obj):\n",
        "\n",
        "    b_tj = self.success_belief(theta, obj)\n",
        "    rS = REWARD_TABLE[(obj, OutcomeType.SP_SUCCESS)]\n",
        "    rF = REWARD_TABLE[(obj, OutcomeType.SP_FAIL)]\n",
        "\n",
        "    return sigmoid(b_tj * rS + (1 - b_tj) * rF)\n",
        "\n",
        "  # now we can calculaute the human action!\n",
        "  def sample_human_action(self, theta, obj):\n",
        "\n",
        "    p_stay = self.prob_stay_put(theta, obj)\n",
        "    if np.random.rand() < p_stay:\n",
        "      return None\n",
        "    else: return OutcomeType.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtvSZsE20NLq"
      },
      "outputs": [],
      "source": [
        "human_model = HumanDecisionModel()\n",
        "\n",
        "# bayesion inference for the probability of each observation\n",
        "def belief(state_theta, action_obj, observation):\n",
        "\n",
        "    p_stay = human_model.prob_stay_put(state_theta, action_obj)\n",
        "\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        return p_stay\n",
        "    else:  # observation == OutcomeType.IT (OutcomeType.Fail isn't possible yet)\n",
        "        return 1 - p_stay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16qr31Uu4U4S"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def belief_dist(theta_t, action, observation, theta_next=None, alpha=1, beta=.2, sigma=1):\n",
        "\n",
        "    # if the robot succeeds\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        mean = theta_t + alpha  # trust increases\n",
        "    else:  # otherwise, trust decreases slightly (by beta)\n",
        "        mean = theta_t - beta\n",
        "\n",
        "    # calculate probabilities for all trust levels\n",
        "    # ex. if the robot succeeds, higher trust level becomes more probably\n",
        "    probs = {}\n",
        "    for theta in TRUST_LEVELS:\n",
        "        lower = theta - 0.5\n",
        "        upper = theta + 0.5\n",
        "        probs[theta] = norm.cdf(upper, loc=mean, scale=sigma) - norm.cdf(lower, loc=mean, scale=sigma)\n",
        "\n",
        "    total = sum(probs.values())\n",
        "    probs = {theta: p / total for theta, p in probs.items()}\n",
        "\n",
        "    if theta_next is not None:\n",
        "        return probs[theta_next]\n",
        "    else:\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0vCxyptn1vQ"
      },
      "outputs": [],
      "source": [
        "def belief_update(belief, action, observation, human_model):\n",
        "\n",
        "    predicted = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        predicted[theta_next] = 0.0\n",
        "        for theta_curr in TRUST_LEVELS:\n",
        "            prior = belief[theta_curr]\n",
        "            trans_prob = belief_dist(theta_curr, action, observation)[theta_next]\n",
        "            predicted[theta_next] += prior * trans_prob\n",
        "\n",
        "    new_belief = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        if observation == OutcomeType.SP_SUCCESS:\n",
        "            obs_prob = human_model.prob_stay_put(theta_next, action)\n",
        "        else:\n",
        "            obs_prob = 1 - human_model.prob_stay_put(theta_next, action)\n",
        "\n",
        "        new_belief[theta_next] = predicted[theta_next] * obs_prob\n",
        "\n",
        "    total = sum(new_belief.values())\n",
        "    new_belief = {theta: p / total for theta, p in new_belief.items()}\n",
        "\n",
        "    return new_belief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwNPIreDEmY1"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "def simple_trust_aware_policy(belief, human_model, available_actions):\n",
        "    \"\"\"\n",
        "    Full-horizon trust-aware planning with memoization.\n",
        "    Explores all trajectories until remaining_objects is empty.\n",
        "    Returns (best_action, best_total_expected_reward).\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert initial belief to tuple (because dicts aren't hashable)\n",
        "    init_belief_tuple = tuple(belief[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    # Convert available_actions to a tuple of integers (ObjectType values)\n",
        "    init_actions_tuple = tuple(sorted([obj.value for obj in available_actions]))\n",
        "\n",
        "    # ---- 1. One-step expected reward (unchanged) ----\n",
        "    def immediate_expected_reward(belief_state, action_obj):\n",
        "        expected = 0.0\n",
        "        for theta in TRUST_LEVELS:\n",
        "            p_theta = belief_state[theta]\n",
        "            p_stay = human_model.prob_stay_put(theta, action_obj)\n",
        "\n",
        "            reward_if_stay = REWARD_TABLE[(action_obj, OutcomeType.SP_SUCCESS)]\n",
        "            reward_if_intervene = REWARD_TABLE[(action_obj, OutcomeType.IT)]\n",
        "\n",
        "            expected += p_theta * (\n",
        "                p_stay * reward_if_stay +\n",
        "                (1 - p_stay) * reward_if_intervene\n",
        "            )\n",
        "        return expected\n",
        "\n",
        "    # ---- 2. Cached wrapper for belief_update ----\n",
        "    @lru_cache(maxsize=None)\n",
        "    def cached_belief_update(belief_tuple, action_value, obs_value):\n",
        "        belief_dict = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "        action_obj = ObjectType(action_value)\n",
        "        obs = OutcomeType(obs_value)\n",
        "        new_b = belief_update(belief_dict, action_obj, obs, human_model)\n",
        "        return tuple(new_b[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    # ---- 3. Main recursive value function, fully memoized ----\n",
        "    @lru_cache(maxsize=None)\n",
        "    def expected_return(belief_tuple, actions_tuple):\n",
        "        \"\"\"\n",
        "        belief_tuple: tuple of 7 floats (belief over trust levels)\n",
        "        actions_tuple: sorted tuple of remaining object values (ints)\n",
        "        \"\"\"\n",
        "\n",
        "        # Base case: no objects left\n",
        "        if not actions_tuple:\n",
        "            return (None, 0.0)\n",
        "\n",
        "        # Convert tuple → usable dict\n",
        "        belief_state = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "\n",
        "        best_act = None\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        # Try each remaining action\n",
        "        for idx, action_value in enumerate(actions_tuple):\n",
        "            action_obj = ObjectType(action_value)\n",
        "\n",
        "            # 1-step reward\n",
        "            immediate = immediate_expected_reward(belief_state, action_obj)\n",
        "\n",
        "            # Expected future reward\n",
        "            future = 0.0\n",
        "            for obs in (OutcomeType.SP_SUCCESS, OutcomeType.IT):\n",
        "                obs_value = obs.value\n",
        "\n",
        "                # Compute P(obs | belief, action)\n",
        "                if obs == OutcomeType.SP_SUCCESS:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] * human_model.prob_stay_put(t, action_obj)\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                else:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] * (1 - human_model.prob_stay_put(t, action_obj))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "\n",
        "                if p_obs == 0:\n",
        "                    continue\n",
        "\n",
        "                # Updated belief (cached!)\n",
        "                next_b_tuple = cached_belief_update(belief_tuple, action_value, obs_value)\n",
        "\n",
        "                # Remove this action from remaining list\n",
        "                next_actions = list(actions_tuple)\n",
        "                next_actions.pop(idx)\n",
        "                next_actions_tuple = tuple(sorted(next_actions))\n",
        "\n",
        "                # Recursive call (cached!)\n",
        "                _, v_next = expected_return(next_b_tuple, next_actions_tuple)\n",
        "\n",
        "                future += p_obs * v_next\n",
        "\n",
        "            total = immediate + future\n",
        "\n",
        "            if total > best_val:\n",
        "                best_val = total\n",
        "                best_act = action_obj\n",
        "\n",
        "        return (best_act, best_val)\n",
        "\n",
        "    # ---- Call planner ----\n",
        "    return expected_return(init_belief_tuple, init_actions_tuple)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv0TtyvUEtaA"
      },
      "outputs": [],
      "source": [
        "def simulate_task(human_model, initial_belief, alpha=0.3, beta=0.0, sigma=0.5, initial_theta=None):\n",
        "    \"\"\"\n",
        "    Simulate a full table-clearing task with DYNAMIC trust.\n",
        "\n",
        "    true_theta_t evolves according to belief_dist(theta_t, action, observation).\n",
        "    \"\"\"\n",
        "    # Belief over trust (robot's internal state)\n",
        "    belief = initial_belief.copy()\n",
        "    total_reward = 0.0\n",
        "    history = []\n",
        "\n",
        "    # Hidden true trust state\n",
        "    if initial_theta is None:\n",
        "        # sample initial trust from prior belief\n",
        "        true_theta = np.random.choice(TRUST_LEVELS, p=list(belief.values()))\n",
        "    else:\n",
        "        true_theta = initial_theta\n",
        "\n",
        "    remaining_objects = [\n",
        "        ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "        ObjectType.CAN, ObjectType.GLASS\n",
        "    ]\n",
        "\n",
        "    while remaining_objects:\n",
        "        # Robot chooses action from its belief\n",
        "        action, expected_reward = simple_trust_aware_policy(belief, human_model, remaining_objects)\n",
        "\n",
        "        # # Make sure action is available\n",
        "        # if action not in remaining_objects:\n",
        "        #     for obj in [ObjectType.BOTTLE, ObjectType.CAN, ObjectType.GLASS]:\n",
        "        #         if obj in remaining_objects:\n",
        "        #             action = obj\n",
        "        #             break\n",
        "\n",
        "        # --- ENVIRONMENT STEP (uses true_theta_t) ---\n",
        "        # Human decides whether to intervene\n",
        "        observation = human_model.sample_human_action(true_theta, action)\n",
        "        if observation is None:\n",
        "            observation = OutcomeType.SP_SUCCESS\n",
        "\n",
        "        # Reward from outcome\n",
        "        reward = REWARD_TABLE[(action, observation)]\n",
        "        total_reward += reward\n",
        "\n",
        "        # --- TRUE TRUST DYNAMICS: theta_{t+1} ---\n",
        "        trans_probs = belief_dist(true_theta, action, observation)\n",
        "        true_theta = np.random.choice(\n",
        "            TRUST_LEVELS,\n",
        "            p=[trans_probs[theta] for theta in TRUST_LEVELS]\n",
        "        )\n",
        "\n",
        "        # --- BELIEF UPDATE (robot’s update about trust) ---\n",
        "        belief = belief_update(belief, action, observation, human_model)\n",
        "\n",
        "        # Task progress\n",
        "        remaining_objects.remove(action)\n",
        "\n",
        "        # record also true_theta for debugging\n",
        "        history.append((action, observation, belief.copy(), true_theta))\n",
        "\n",
        "    return total_reward, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsngMO2fGyyE"
      },
      "outputs": [],
      "source": [
        "def run_multiple_simulations(human_model, true_theta, initial_belief, n_runs=100):\n",
        "    all_rewards = []\n",
        "    action_sequences = []\n",
        "    first_action_counts = {\n",
        "        ObjectType.BOTTLE: 0,\n",
        "        ObjectType.CAN: 0,\n",
        "        ObjectType.GLASS: 0\n",
        "    }\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        total_reward, history = simulate_task(\n",
        "            human_model,\n",
        "            initial_belief,\n",
        "            initial_theta=true_theta  # <-- now used!\n",
        "        )\n",
        "        all_rewards.append(total_reward)\n",
        "\n",
        "        action_seq = [action.name for (action, obs, belief, theta) in history]\n",
        "        action_sequences.append(action_seq)\n",
        "\n",
        "        first_action = history[0][0]\n",
        "        first_action_counts[first_action] += 1\n",
        "\n",
        "    print(f\"Average reward: {np.mean(all_rewards):.2f} ± {np.std(all_rewards):.2f}\")\n",
        "    print(f\"\\nFirst action distribution (out of {n_runs} runs):\")\n",
        "    for obj, count in first_action_counts.items():\n",
        "        print(f\"  {obj.name}: {count} times ({100*count/n_runs:.1f}%)\")\n",
        "\n",
        "    from collections import Counter\n",
        "    seq_counts = Counter([tuple(seq) for seq in action_sequences])\n",
        "    print(f\"\\nMost common action sequences:\")\n",
        "    for seq, count in seq_counts.most_common(5):\n",
        "        print(f\"  {' → '.join(seq)}: {count} times\")\n",
        "\n",
        "    return all_rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG4Sd-OXG4nO",
        "outputId": "34ed5d50-3382-4e4e-ab90-857b27307b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: 8.00 ± 0.00\n",
            "\n",
            "First action distribution (out of 1 runs):\n",
            "  BOTTLE: 1 times (100.0%)\n",
            "  CAN: 0 times (0.0%)\n",
            "  GLASS: 0 times (0.0%)\n",
            "\n",
            "Most common action sequences:\n",
            "  BOTTLE → BOTTLE → BOTTLE → CAN → GLASS: 1 times\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8.0]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Initialize human model (you'll need to set gamma and eta)\n",
        "human_model = HumanDecisionModel()\n",
        "\n",
        "human_model.gamma = {\n",
        "    ObjectType.BOTTLE: 1,   # moderate slope\n",
        "    ObjectType.CAN:    1.2,   # sharp slope, only grows at medium-high θ\n",
        "    ObjectType.GLASS:  1.5,   # very sharp slope, requires high θ\n",
        "}\n",
        "\n",
        "human_model.eta = {\n",
        "    ObjectType.BOTTLE: 0,   # threshold ~ 1.6\n",
        "    ObjectType.CAN:   -2.5,   # threshold ~ 4.0\n",
        "    ObjectType.GLASS: -5,   # threshold ~ 6.0\n",
        "}\n",
        "\n",
        "initial_belief =   {\n",
        "    1: 0.0625,\n",
        "    2: 0.125,\n",
        "    3: 0.1825,\n",
        "    4: 0.375,\n",
        "    5: 0.1825,\n",
        "    6: 0.125,\n",
        "    7: 0.0625\n",
        "}\n",
        "\n",
        "run_multiple_simulations(\n",
        "    human_model,\n",
        "    true_theta=4,\n",
        "    initial_belief=initial_belief,\n",
        "    n_runs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for theta in TRUST_LEVELS:\n",
        "    print(theta,\n",
        "          human_model.prob_stay_put(theta, ObjectType.BOTTLE),\n",
        "          human_model.prob_stay_put(theta, ObjectType.CAN),\n",
        "          human_model.prob_stay_put(theta, ObjectType.GLASS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JSVYmGDY1mM",
        "outputId": "37265662-eda4-472e-e512-2c84702e1727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0.6750375273768237 0.06209344479271049 0.0001754022502924929\n",
            "2 0.7069873680001046 0.2405118927418583 0.0005156508238289689\n",
            "3 0.7216325609518421 0.6228260157313329 0.011323220046987948\n",
            "4 0.7275076135036415 0.8104999872916171 0.44341912568990316\n",
            "5 0.729740651093834 0.8610605518001789 0.889898215476072\n",
            "6 0.7305721537541839 0.8750011002019935 0.9418141344568051\n",
            "7 0.7308794173912335 0.8790652768279943 0.9503182066279046\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}