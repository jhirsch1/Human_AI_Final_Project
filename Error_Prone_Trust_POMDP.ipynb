{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3RVSe0IFrG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import Enum, auto\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bIRUqpElMsy"
      },
      "outputs": [],
      "source": [
        "# helpful enums\n",
        "\n",
        "# object types - bottle, can, or glass\n",
        "class ObjectType(Enum):\n",
        "    BOTTLE = 1\n",
        "    CAN = 2\n",
        "    GLASS = 3\n",
        "\n",
        "# outcomes\n",
        "class OutcomeType(Enum):\n",
        "    SP_SUCCESS = 0 # robot succeeds\n",
        "    SP_FAIL = 1 # robot fails\n",
        "    IT = 2 # human intervenes\n",
        "\n",
        "TRUST_LEVELS = [1,2,3,4,5,6,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc_rrkUolvOp"
      },
      "outputs": [],
      "source": [
        "# reward table\n",
        "\n",
        "REWARD_TABLE: Dict[Tuple[ObjectType, OutcomeType], float] = {\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_SUCCESS): 1.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_FAIL):    0.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.IT):         0.0,\n",
        "\n",
        "    (ObjectType.CAN, OutcomeType.SP_SUCCESS):    2.0,\n",
        "    (ObjectType.CAN, OutcomeType.SP_FAIL):      -4.0,\n",
        "    (ObjectType.CAN, OutcomeType.IT):            0.0,\n",
        "\n",
        "    (ObjectType.GLASS, OutcomeType.SP_SUCCESS):  3.0,\n",
        "    (ObjectType.GLASS, OutcomeType.SP_FAIL):    -12.0,\n",
        "    (ObjectType.GLASS, OutcomeType.IT):          0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Intrinsic robot failure rates for each object type\n",
        "ROBOT_FAIL_RATE = {\n",
        "    ObjectType.BOTTLE: 0.00,\n",
        "    ObjectType.CAN:    0.00,\n",
        "    ObjectType.GLASS:  0.25,\n",
        "}\n"
      ],
      "metadata": {
        "id": "c2YxtwtpSdIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gQt-AkTl4O8"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class HumanDecisionModel:\n",
        "\n",
        "  gamma: Dict[ObjectType, float]\n",
        "  eta: Dict[ObjectType, float]\n",
        "\n",
        "  # does the human think the robot will succeed?\n",
        "  def success_belief(self, theta, obj):\n",
        "\n",
        "    g = self.gamma[obj]\n",
        "    n = self.eta[obj]\n",
        "\n",
        "    return sigmoid(g * theta + n)\n",
        "\n",
        "  # what is the probability the human will stay put / not intervene?\n",
        "  def prob_stay_put(self, theta, obj):\n",
        "\n",
        "    b_tj = self.success_belief(theta, obj)\n",
        "    rS = REWARD_TABLE[(obj, OutcomeType.SP_SUCCESS)]\n",
        "    rF = REWARD_TABLE[(obj, OutcomeType.SP_FAIL)]\n",
        "\n",
        "    return sigmoid(b_tj * rS + (1 - b_tj) * rF)\n",
        "\n",
        "  def sample_human_action(self, theta, obj):\n",
        "\n",
        "    p_stay = self.prob_stay_put(theta, obj)\n",
        "    if np.random.rand() < p_stay:\n",
        "      return None\n",
        "    else: return OutcomeType.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtvSZsE20NLq"
      },
      "outputs": [],
      "source": [
        "human_model = HumanDecisionModel()\n",
        "\n",
        "def belief(state_theta, action_obj, observation):\n",
        "\n",
        "    p_stay = human_model.prob_stay_put(state_theta, action_obj)\n",
        "    p_fail = ROBOT_FAIL_RATE[action_obj]\n",
        "    p_succ_given_no_intervention = 1 - p_fail\n",
        "\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        return p_stay * p_succ_given_no_intervention\n",
        "\n",
        "    elif observation == OutcomeType.SP_FAIL:\n",
        "        return p_stay * p_fail\n",
        "\n",
        "    else:\n",
        "        return 1 - p_stay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16qr31Uu4U4S"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def belief_dist(theta_t, action, observation, theta_next=None, alpha=1.0, beta=0.2, epsilon=1.0, sigma=1):\n",
        "    \"\"\"\n",
        "    P(θ_{t+1} | θ_t, a_t, o_t)\n",
        "\n",
        "    - If robot succeeds: trust increases by +alpha\n",
        "    - If robot fails:    trust decreases by -beta\n",
        "    - If human intervenes (IT): trust stays the same\n",
        "    \"\"\"\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        mean = theta_t + alpha\n",
        "    elif observation == OutcomeType.SP_FAIL:\n",
        "        mean = theta_t - epsilon\n",
        "    else:\n",
        "        mean = theta_t - beta\n",
        "\n",
        "    probs = {}\n",
        "    for theta in TRUST_LEVELS:\n",
        "        lower = theta - 0.5\n",
        "        upper = theta + 0.5\n",
        "        probs[theta] = norm.cdf(upper, loc=mean, scale=sigma) - norm.cdf(lower, loc=mean, scale=sigma)\n",
        "\n",
        "    total = sum(probs.values())\n",
        "    probs = {theta: p / total for theta, p in probs.items()}\n",
        "\n",
        "    if theta_next is not None:\n",
        "        return probs[theta_next]\n",
        "    else:\n",
        "        return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0vCxyptn1vQ"
      },
      "outputs": [],
      "source": [
        "def belief_update(belief, action, observation, human_model):\n",
        "\n",
        "    predicted = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        predicted[theta_next] = 0.0\n",
        "        for theta_curr in TRUST_LEVELS:\n",
        "            prior = belief[theta_curr]\n",
        "            trans_prob = belief_dist(theta_curr, action, observation)[theta_next]\n",
        "            predicted[theta_next] += prior * trans_prob\n",
        "\n",
        "    new_belief = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "\n",
        "        p_stay = human_model.prob_stay_put(theta_next, action)\n",
        "        p_fail = ROBOT_FAIL_RATE[action]\n",
        "        p_succ = 1 - p_fail\n",
        "\n",
        "        if observation == OutcomeType.SP_SUCCESS:\n",
        "            obs_prob = p_stay * p_succ\n",
        "\n",
        "        elif observation == OutcomeType.SP_FAIL:\n",
        "            obs_prob = p_stay * p_fail\n",
        "\n",
        "        elif observation == OutcomeType.IT:\n",
        "            obs_prob = 1 - p_stay\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Unknown observation\")\n",
        "\n",
        "        new_belief[theta_next] = predicted[theta_next] * obs_prob\n",
        "\n",
        "    # normalize\n",
        "    total = sum(new_belief.values())\n",
        "    new_belief = {theta: p / total for theta, p in new_belief.items()}\n",
        "\n",
        "    return new_belief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwNPIreDEmY1"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "def simple_trust_aware_policy(belief, human_model, available_actions):\n",
        "    \"\"\"\n",
        "    Full-horizon trust-aware planning with memoization.\n",
        "    Explores all trajectories until remaining_objects is empty.\n",
        "    Returns (best_action, best_total_expected_reward).\n",
        "    \"\"\"\n",
        "\n",
        "    init_belief_tuple = tuple(belief[theta] for theta in TRUST_LEVELS)\n",
        "    init_actions_tuple = tuple(sorted([obj.value for obj in available_actions]))\n",
        "\n",
        "    def immediate_expected_reward(belief_state, action_obj):\n",
        "        expected = 0.0\n",
        "        fail_rate = ROBOT_FAIL_RATE[action_obj]\n",
        "\n",
        "        for theta in TRUST_LEVELS:\n",
        "            p_theta = belief_state[theta]\n",
        "            p_stay = human_model.prob_stay_put(theta, action_obj)\n",
        "\n",
        "            r_success = REWARD_TABLE[(action_obj, OutcomeType.SP_SUCCESS)]\n",
        "            r_fail    = REWARD_TABLE[(action_obj, OutcomeType.SP_FAIL)]\n",
        "            r_it      = REWARD_TABLE[(action_obj, OutcomeType.IT)]\n",
        "\n",
        "            # Outcome probabilities given theta, action\n",
        "            p_IT   = 1 - p_stay\n",
        "            p_fail = p_stay * fail_rate\n",
        "            p_succ = p_stay * (1 - fail_rate)\n",
        "\n",
        "            one_step = (\n",
        "                p_succ * r_success +\n",
        "                p_fail * r_fail +\n",
        "                p_IT   * r_it\n",
        "            )\n",
        "\n",
        "            expected += p_theta * one_step\n",
        "\n",
        "        return expected\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def cached_belief_update(belief_tuple, action_value, obs_value):\n",
        "        belief_dict = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "        action_obj = ObjectType(action_value)\n",
        "        obs = OutcomeType(obs_value)\n",
        "        new_b = belief_update(belief_dict, action_obj, obs, human_model)\n",
        "        return tuple(new_b[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def expected_return(belief_tuple, actions_tuple):\n",
        "        if not actions_tuple:\n",
        "            return (None, 0.0)\n",
        "\n",
        "        belief_state = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "\n",
        "        best_act = ObjectType(actions_tuple[0])\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        for idx, action_value in enumerate(actions_tuple):\n",
        "            action_obj = ObjectType(action_value)\n",
        "\n",
        "            # One-step reward\n",
        "            immediate = immediate_expected_reward(belief_state, action_obj)\n",
        "\n",
        "            # Expected future reward\n",
        "            future = 0.0\n",
        "            for obs in (OutcomeType.SP_SUCCESS, OutcomeType.SP_FAIL, OutcomeType.IT):\n",
        "                obs_value = obs.value\n",
        "\n",
        "                if obs == OutcomeType.SP_SUCCESS:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (human_model.prob_stay_put(t, action_obj) *\n",
        "                         (1 - ROBOT_FAIL_RATE[action_obj]))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                elif obs == OutcomeType.SP_FAIL:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (human_model.prob_stay_put(t, action_obj) *\n",
        "                         ROBOT_FAIL_RATE[action_obj])\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                elif obs == OutcomeType.IT:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (1 - human_model.prob_stay_put(t, action_obj))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "\n",
        "                if p_obs == 0.0:\n",
        "                    continue\n",
        "\n",
        "                # Next belief (cached)\n",
        "                next_b_tuple = cached_belief_update(belief_tuple, action_value, obs_value)\n",
        "\n",
        "                # Next actions (remove current)\n",
        "                next_actions = list(actions_tuple)\n",
        "                next_actions.pop(idx)\n",
        "                next_actions_tuple = tuple(sorted(next_actions))\n",
        "\n",
        "                _, v_next = expected_return(next_b_tuple, next_actions_tuple)\n",
        "\n",
        "                future += p_obs * v_next\n",
        "\n",
        "            total = immediate + future\n",
        "\n",
        "            if total > best_val:\n",
        "                best_val = total\n",
        "                best_act = action_obj\n",
        "\n",
        "        return (best_act, best_val)\n",
        "\n",
        "    return expected_return(init_belief_tuple, init_actions_tuple)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv0TtyvUEtaA"
      },
      "outputs": [],
      "source": [
        "def simulate_task(human_model, initial_belief, initial_theta=None):\n",
        "    \"\"\"\n",
        "    Simulate a full table-clearing task with DYNAMIC trust.\n",
        "    true_theta_t evolves according to belief_dist(theta_t, action, observation).\n",
        "    \"\"\"\n",
        "\n",
        "    belief = initial_belief.copy()\n",
        "    total_reward = 0.0\n",
        "    history = []\n",
        "\n",
        "    # Hidden true trust state\n",
        "    if initial_theta is None:\n",
        "        true_theta = np.random.choice(TRUST_LEVELS, p=list(belief.values()))\n",
        "    else:\n",
        "        true_theta = initial_theta\n",
        "\n",
        "    remaining_objects = [\n",
        "        ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "        ObjectType.CAN, ObjectType.GLASS\n",
        "    ]\n",
        "\n",
        "    while remaining_objects:\n",
        "        # 1) Robot chooses action from its *belief*\n",
        "        action, expected_reward = simple_trust_aware_policy(\n",
        "            belief, human_model, remaining_objects\n",
        "        )\n",
        "\n",
        "        # 2) Environment step: intrinsic robot failure THEN human response\n",
        "        fail_rate = ROBOT_FAIL_RATE[action]\n",
        "        rand = np.random.rand()\n",
        "\n",
        "        if rand < fail_rate:\n",
        "            observation = OutcomeType.SP_FAIL\n",
        "        else:\n",
        "            human_obs = human_model.sample_human_action(true_theta, action)\n",
        "            if human_obs is None:\n",
        "                observation = OutcomeType.SP_SUCCESS\n",
        "            else:\n",
        "                observation = OutcomeType.IT\n",
        "\n",
        "        # 3) Reward\n",
        "        reward = REWARD_TABLE[(action, observation)]\n",
        "        total_reward += reward\n",
        "\n",
        "        # 4) True trust dynamics\n",
        "        trans_probs = belief_dist(true_theta, action, observation)\n",
        "        true_theta = np.random.choice(\n",
        "            TRUST_LEVELS,\n",
        "            p=[trans_probs[theta] for theta in TRUST_LEVELS]\n",
        "        )\n",
        "\n",
        "        # 5) Robot's belief update\n",
        "        belief = belief_update(belief, action, observation, human_model)\n",
        "\n",
        "        # 6) Remove object and log\n",
        "        remaining_objects.remove(action)\n",
        "        history.append((action, observation, belief.copy(), true_theta))\n",
        "\n",
        "    return total_reward, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsngMO2fGyyE"
      },
      "outputs": [],
      "source": [
        "def run_multiple_simulations(human_model, true_theta, initial_belief, n_runs=100):\n",
        "    all_rewards = []\n",
        "    action_sequences = []\n",
        "    first_action_counts = {\n",
        "        ObjectType.BOTTLE: 0,\n",
        "        ObjectType.CAN: 0,\n",
        "        ObjectType.GLASS: 0\n",
        "    }\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "        total_reward, history = simulate_task(\n",
        "            human_model,\n",
        "            initial_belief,\n",
        "            initial_theta=true_theta  # <-- now used!\n",
        "        )\n",
        "        all_rewards.append(total_reward)\n",
        "\n",
        "        action_seq = [action.name for (action, obs, belief, theta) in history]\n",
        "        action_sequences.append(action_seq)\n",
        "\n",
        "        first_action = history[0][0]\n",
        "        first_action_counts[first_action] += 1\n",
        "\n",
        "    print(f\"Average reward: {np.mean(all_rewards):.2f} ± {np.std(all_rewards):.2f}\")\n",
        "    print(f\"\\nFirst action distribution (out of {n_runs} runs):\")\n",
        "    for obj, count in first_action_counts.items():\n",
        "        print(f\"  {obj.name}: {count} times ({100*count/n_runs:.1f}%)\")\n",
        "\n",
        "    from collections import Counter\n",
        "    seq_counts = Counter([tuple(seq) for seq in action_sequences])\n",
        "    print(f\"\\nMost common action sequences:\")\n",
        "    for seq, count in seq_counts.most_common(5):\n",
        "        print(f\"  {' → '.join(seq)}: {count} times\")\n",
        "\n",
        "    return all_rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG4Sd-OXG4nO",
        "outputId": "bd7cf7c3-fd3e-46c4-cb4e-acf8beee10fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: 1.00 ± 0.00\n",
            "\n",
            "First action distribution (out of 1 runs):\n",
            "  BOTTLE: 0 times (0.0%)\n",
            "  CAN: 0 times (0.0%)\n",
            "  GLASS: 1 times (100.0%)\n",
            "\n",
            "Most common action sequences:\n",
            "  GLASS → BOTTLE → BOTTLE → BOTTLE → CAN: 1 times\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Initialize human model (you'll need to set gamma and eta)\n",
        "human_model = HumanDecisionModel()\n",
        "\n",
        "human_model.gamma = {\n",
        "    ObjectType.BOTTLE: 1,   # moderate slope\n",
        "    ObjectType.CAN:    1.2,   # sharp slope, only grows at medium-high θ\n",
        "    ObjectType.GLASS:  1.5,   # very sharp slope, requires high θ\n",
        "}\n",
        "\n",
        "human_model.eta = {\n",
        "    ObjectType.BOTTLE: 0,   # threshold ~ 1.6\n",
        "    ObjectType.CAN:   -2.5,   # threshold ~ 4.0\n",
        "    ObjectType.GLASS: -5,   # threshold ~ 6.0\n",
        "}\n",
        "\n",
        "initial_belief =   {\n",
        "    1: 0.0625,\n",
        "    2: 0.125,\n",
        "    3: 0.1825,\n",
        "    4: 0.375,\n",
        "    5: 0.1825,\n",
        "    6: 0.125,\n",
        "    7: 0.0625\n",
        "}\n",
        "\n",
        "available = [\n",
        "    ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "    ObjectType.CAN, ObjectType.GLASS\n",
        "]\n",
        "\n",
        "# best_act, best_val = simple_trust_aware_policy(initial_belief, human_model, available)\n",
        "# print(\"Best first action:\", best_act, \"with value\", best_val)\n",
        "\n",
        "\n",
        "run_multiple_simulations(\n",
        "    human_model,\n",
        "    true_theta=4,\n",
        "    initial_belief=initial_belief,\n",
        "    n_runs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for theta in TRUST_LEVELS:\n",
        "    print(theta,\n",
        "          human_model.prob_stay_put(theta, ObjectType.BOTTLE),\n",
        "          human_model.prob_stay_put(theta, ObjectType.CAN),\n",
        "          human_model.prob_stay_put(theta, ObjectType.GLASS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JSVYmGDY1mM",
        "outputId": "f0390fba-5e8f-4ab5-fdfc-6dcd55b50e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0.6750375273768237 0.06209344479271049 9.537052563699948e-06\n",
            "2 0.7069873680001046 0.2405118927418583 3.67272013022538e-05\n",
            "3 0.7216325609518421 0.6228260157313329 0.00176667116371655\n",
            "4 0.7275076135036415 0.8104999872916172 0.26228588073449527\n",
            "5 0.729740651093834 0.8610605518001789 0.8655447095136148\n",
            "6 0.7305721537541839 0.8750011002019935 0.9387857253587264\n",
            "7 0.7308794173912335 0.8790652768279943 0.9497385299392656\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}