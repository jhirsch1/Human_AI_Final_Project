{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s3RVSe0IFrG8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from enum import Enum\n",
        "from typing import Dict, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_bIRUqpElMsy"
      },
      "outputs": [],
      "source": [
        "# helpful enums\n",
        "\n",
        "# object types - bottle, can, or glass\n",
        "class ObjectType(Enum):\n",
        "    BOTTLE = 1\n",
        "    CAN = 2\n",
        "    GLASS = 3\n",
        "\n",
        "# outcomes\n",
        "class OutcomeType(Enum):\n",
        "    SP_SUCCESS = 0 # robot succeeds\n",
        "    SP_FAIL = 1 # robot fails\n",
        "    IT = 2 # human intervenes\n",
        "\n",
        "TRUST_LEVELS = [1,2,3,4,5,6,7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nc_rrkUolvOp"
      },
      "outputs": [],
      "source": [
        "# reward table\n",
        "\n",
        "REWARD_TABLE: Dict[Tuple[ObjectType, OutcomeType], float] = {\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_SUCCESS): 1.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.SP_FAIL):    0.0,\n",
        "    (ObjectType.BOTTLE, OutcomeType.IT):         0.0,\n",
        "\n",
        "    (ObjectType.CAN, OutcomeType.SP_SUCCESS):    2.0,\n",
        "    (ObjectType.CAN, OutcomeType.SP_FAIL):      -4.0,\n",
        "    (ObjectType.CAN, OutcomeType.IT):            0.0,\n",
        "\n",
        "    (ObjectType.GLASS, OutcomeType.SP_SUCCESS):  3.0,\n",
        "    (ObjectType.GLASS, OutcomeType.SP_FAIL):    -12.0,\n",
        "    (ObjectType.GLASS, OutcomeType.IT):          0.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c2YxtwtpSdIO"
      },
      "outputs": [],
      "source": [
        "# adding failure rate\n",
        "ROBOT_FAIL_RATE = {\n",
        "    ObjectType.BOTTLE: 0.00,\n",
        "    ObjectType.CAN:    0.00,\n",
        "    ObjectType.GLASS:  0.25,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6gQt-AkTl4O8"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class HumanDecisionModel:\n",
        "\n",
        "  gamma: Dict[ObjectType, float]\n",
        "  eta: Dict[ObjectType, float]\n",
        "\n",
        "  # does the human think the robot will succeed?\n",
        "  def success_belief(self, theta, obj):\n",
        "\n",
        "    g = self.gamma[obj]\n",
        "    n = self.eta[obj]\n",
        "\n",
        "    return sigmoid(g * theta + n)\n",
        "\n",
        "  # what is the probability the human will stay put / not intervene?\n",
        "  def prob_stay_put(self, theta, obj):\n",
        "\n",
        "    b_tj = self.success_belief(theta, obj)\n",
        "    rS = REWARD_TABLE[(obj, OutcomeType.SP_SUCCESS)]\n",
        "    rF = REWARD_TABLE[(obj, OutcomeType.SP_FAIL)]\n",
        "\n",
        "    return sigmoid(b_tj * rS + (1 - b_tj) * rF)\n",
        "\n",
        "  def sample_human_action(self, theta, obj):\n",
        "\n",
        "    p_stay = self.prob_stay_put(theta, obj)\n",
        "    if np.random.rand() < p_stay:\n",
        "      return None\n",
        "    else: return OutcomeType.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JtvSZsE20NLq"
      },
      "outputs": [],
      "source": [
        "human_model = HumanDecisionModel()\n",
        "\n",
        "# bayesion inference for the probability of each observation\n",
        "def belief(state_theta, action_obj, observation):\n",
        "\n",
        "    p_stay = human_model.prob_stay_put(state_theta, action_obj)\n",
        "    p_fail = ROBOT_FAIL_RATE[action_obj]\n",
        "\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        return p_stay * (1 - p_fail)\n",
        "\n",
        "    elif observation == OutcomeType.SP_FAIL:\n",
        "        return p_stay * p_fail\n",
        "\n",
        "    else:\n",
        "        return 1 - p_stay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "16qr31Uu4U4S"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def belief_dist(theta_t, observation, theta_next=None, alpha=1.0, beta=0.2, epsilon=1.0, sigma=1):\n",
        "    \"\"\"\n",
        "    P(θ_{t+1} | θ_t, a_t, o_t)\n",
        "\n",
        "    - if robot succeeds:      trust increases by +alpha\n",
        "    - if robot fails:         trust decreases by -beta\n",
        "    - if human intervenes:    trust stays the same\n",
        "    \"\"\"\n",
        "    if observation == OutcomeType.SP_SUCCESS:\n",
        "        mean = theta_t + alpha\n",
        "    elif observation == OutcomeType.SP_FAIL:\n",
        "        mean = theta_t - epsilon\n",
        "    else:\n",
        "        mean = theta_t - beta\n",
        "\n",
        "    probs = {}\n",
        "    for theta in TRUST_LEVELS:\n",
        "        lower = theta - 0.5\n",
        "        upper = theta + 0.5\n",
        "        probs[theta] = norm.cdf(upper, loc=mean, scale=sigma) - norm.cdf(lower, loc=mean, scale=sigma)\n",
        "\n",
        "    total = sum(probs.values())\n",
        "    probs = {theta: p / total for theta, p in probs.items()}\n",
        "\n",
        "    if theta_next is not None:\n",
        "        return probs[theta_next]\n",
        "    else:\n",
        "        return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u0vCxyptn1vQ"
      },
      "outputs": [],
      "source": [
        "def belief_update(belief, action, observation, human_model):\n",
        "\n",
        "    predicted = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "        predicted[theta_next] = 0.0\n",
        "        for theta_curr in TRUST_LEVELS:\n",
        "            prior = belief[theta_curr]\n",
        "            trans_prob = belief_dist(theta_curr, observation)[theta_next]\n",
        "            predicted[theta_next] += prior * trans_prob\n",
        "\n",
        "    new_belief = {}\n",
        "    for theta_next in TRUST_LEVELS:\n",
        "\n",
        "        p_stay = human_model.prob_stay_put(theta_next, action)\n",
        "        p_fail = ROBOT_FAIL_RATE[action]\n",
        "        p_succ = 1 - p_fail\n",
        "\n",
        "        if observation == OutcomeType.SP_SUCCESS:\n",
        "            obs_prob = p_stay * p_succ\n",
        "\n",
        "        elif observation == OutcomeType.SP_FAIL:\n",
        "            obs_prob = p_stay * p_fail\n",
        "\n",
        "        else:\n",
        "            obs_prob = 1 - p_stay\n",
        "\n",
        "        new_belief[theta_next] = predicted[theta_next] * obs_prob\n",
        "\n",
        "    # normalize\n",
        "    total = sum(new_belief.values())\n",
        "    new_belief = {theta: p / total for theta, p in new_belief.items()}\n",
        "\n",
        "    return new_belief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QwNPIreDEmY1"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "# ChatGPT helped generate an exhaustive tree search in place of using SARSOP (which was incredibly difficult to download/run)\n",
        "# The important distinction here is that, since our POMDP environment is so small, we are able to explore the entire state space\n",
        "# Our results converge to a completely optimal policy (versus SARSOP uses approximation)\n",
        "# This would NOT scale to a larger environment\n",
        "\n",
        "def simple_trust_aware_policy(belief, human_model, available_actions):\n",
        "    \"\"\"\n",
        "    Full-horizon trust-aware planning with memoization.\n",
        "    Explores all trajectories until remaining_objects is empty.\n",
        "    Returns (best_action, best_total_expected_reward).\n",
        "    \"\"\"\n",
        "\n",
        "    init_belief_tuple = tuple(belief[theta] for theta in TRUST_LEVELS)\n",
        "    init_actions_tuple = tuple(sorted([obj.value for obj in available_actions]))\n",
        "\n",
        "    def immediate_expected_reward(belief_state, action_obj):\n",
        "        expected = 0.0\n",
        "        fail_rate = ROBOT_FAIL_RATE[action_obj]\n",
        "\n",
        "        for theta in TRUST_LEVELS:\n",
        "            p_theta = belief_state[theta]\n",
        "            p_stay = human_model.prob_stay_put(theta, action_obj)\n",
        "\n",
        "            r_success = REWARD_TABLE[(action_obj, OutcomeType.SP_SUCCESS)]\n",
        "            r_fail    = REWARD_TABLE[(action_obj, OutcomeType.SP_FAIL)]\n",
        "            r_it      = REWARD_TABLE[(action_obj, OutcomeType.IT)]\n",
        "\n",
        "            # Outcome probabilities given theta, action\n",
        "            p_IT   = 1 - p_stay\n",
        "            p_fail = p_stay * fail_rate\n",
        "            p_succ = p_stay * (1 - fail_rate)\n",
        "\n",
        "            one_step = (\n",
        "                p_succ * r_success +\n",
        "                p_fail * r_fail +\n",
        "                p_IT   * r_it\n",
        "            )\n",
        "\n",
        "            expected += p_theta * one_step\n",
        "\n",
        "        return expected\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def cached_belief_update(belief_tuple, action_value, obs_value):\n",
        "        belief_dict = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "        action_obj = ObjectType(action_value)\n",
        "        obs = OutcomeType(obs_value)\n",
        "        new_b = belief_update(belief_dict, action_obj, obs, human_model)\n",
        "        return tuple(new_b[theta] for theta in TRUST_LEVELS)\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def expected_return(belief_tuple, actions_tuple):\n",
        "        if not actions_tuple:\n",
        "            return (None, 0.0)\n",
        "\n",
        "        belief_state = {theta: belief_tuple[i] for i, theta in enumerate(TRUST_LEVELS)}\n",
        "\n",
        "        best_act = ObjectType(actions_tuple[0])\n",
        "        best_val = -float('inf')\n",
        "\n",
        "        for idx, action_value in enumerate(actions_tuple):\n",
        "            action_obj = ObjectType(action_value)\n",
        "\n",
        "            # One-step reward\n",
        "            immediate = immediate_expected_reward(belief_state, action_obj)\n",
        "\n",
        "            # Expected future reward\n",
        "            future = 0.0\n",
        "            for obs in (OutcomeType.SP_SUCCESS, OutcomeType.SP_FAIL, OutcomeType.IT):\n",
        "                obs_value = obs.value\n",
        "\n",
        "                if obs == OutcomeType.SP_SUCCESS:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (human_model.prob_stay_put(t, action_obj) *\n",
        "                         (1 - ROBOT_FAIL_RATE[action_obj]))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                elif obs == OutcomeType.SP_FAIL:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (human_model.prob_stay_put(t, action_obj) *\n",
        "                         ROBOT_FAIL_RATE[action_obj])\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "                elif obs == OutcomeType.IT:\n",
        "                    p_obs = sum(\n",
        "                        belief_state[t] *\n",
        "                        (1 - human_model.prob_stay_put(t, action_obj))\n",
        "                        for t in TRUST_LEVELS\n",
        "                    )\n",
        "\n",
        "                if p_obs == 0.0:\n",
        "                    continue\n",
        "\n",
        "                # Next belief (cached)\n",
        "                next_b_tuple = cached_belief_update(belief_tuple, action_value, obs_value)\n",
        "\n",
        "                # Next actions (remove current)\n",
        "                next_actions = list(actions_tuple)\n",
        "                next_actions.pop(idx)\n",
        "                next_actions_tuple = tuple(sorted(next_actions))\n",
        "\n",
        "                _, v_next = expected_return(next_b_tuple, next_actions_tuple)\n",
        "\n",
        "                future += p_obs * v_next\n",
        "\n",
        "            total = immediate + future\n",
        "\n",
        "            if total > best_val:\n",
        "                best_val = total\n",
        "                best_act = action_obj\n",
        "\n",
        "        return (best_act, best_val)\n",
        "\n",
        "    return expected_return(init_belief_tuple, init_actions_tuple)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zv0TtyvUEtaA"
      },
      "outputs": [],
      "source": [
        "def simulate_task(human_model, initial_belief, initial_theta):\n",
        "    \"\"\"\n",
        "    simulate a full table-clearing task\n",
        "    \"\"\"\n",
        "\n",
        "    belief = initial_belief.copy()\n",
        "    total_reward = 0.0\n",
        "    history = []\n",
        "\n",
        "    true_theta = initial_theta\n",
        "\n",
        "    remaining_objects = [\n",
        "        ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "        ObjectType.CAN, ObjectType.GLASS\n",
        "    ]\n",
        "\n",
        "    while remaining_objects:\n",
        "\n",
        "        action, _ = simple_trust_aware_policy(belief, human_model, remaining_objects)\n",
        "\n",
        "        fail_rate = ROBOT_FAIL_RATE[action]\n",
        "        rand = np.random.rand()\n",
        "\n",
        "        if rand < fail_rate:\n",
        "            observation = OutcomeType.SP_FAIL\n",
        "        else:\n",
        "            human_obs = human_model.sample_human_action(true_theta, action)\n",
        "            if human_obs is None:\n",
        "                observation = OutcomeType.SP_SUCCESS\n",
        "            else:\n",
        "                observation = OutcomeType.IT\n",
        "\n",
        "        reward = REWARD_TABLE[(action, observation)]\n",
        "        total_reward += reward\n",
        "\n",
        "        trans_probs = belief_dist(true_theta, observation)\n",
        "        true_theta = np.random.choice(\n",
        "            TRUST_LEVELS,\n",
        "            p=[trans_probs[theta] for theta in TRUST_LEVELS]\n",
        "        )\n",
        "\n",
        "        remaining_objects.remove(action)\n",
        "        history.append(action.name)\n",
        "\n",
        "    return total_reward, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG4Sd-OXG4nO",
        "outputId": "bd7cf7c3-fd3e-46c4-cb4e-acf8beee10fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Reward: -10.0\n",
            "Action Sequence: ['GLASS', 'BOTTLE', 'BOTTLE', 'BOTTLE', 'CAN']\n"
          ]
        }
      ],
      "source": [
        "human_model = HumanDecisionModel()\n",
        "\n",
        "human_model.gamma = {\n",
        "    ObjectType.BOTTLE: 1, \n",
        "    ObjectType.CAN:    1.2,   \n",
        "    ObjectType.GLASS:  1.5,  \n",
        "}\n",
        "\n",
        "human_model.eta = {\n",
        "    ObjectType.BOTTLE: 0,  \n",
        "    ObjectType.CAN:   -2.5,   \n",
        "    ObjectType.GLASS: -5,   \n",
        "}\n",
        "\n",
        "initial_belief =   {\n",
        "    1: 0.0625,\n",
        "    2: 0.125,\n",
        "    3: 0.1825,\n",
        "    4: 0.375,\n",
        "    5: 0.1825,\n",
        "    6: 0.125,\n",
        "    7: 0.0625\n",
        "}\n",
        "\n",
        "available = [\n",
        "    ObjectType.BOTTLE, ObjectType.BOTTLE, ObjectType.BOTTLE,\n",
        "    ObjectType.CAN, ObjectType.GLASS\n",
        "]\n",
        "\n",
        "total_reward, history = simulate_task(\n",
        "            human_model,\n",
        "            initial_belief,\n",
        "            4\n",
        ")\n",
        "\n",
        "print(\"Total Reward: \" + str(total_reward))\n",
        "print(\"Action Sequence: \" + str(history))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "csci1470 (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
